{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71cb948f",
   "metadata": {},
   "source": [
    "# Touchdesigner and Mediapipe Integration\n",
    "This documentation is a walkthrough to setting up mediapipe and integrate it with touchdesigner.\n",
    "\n",
    "## Requirements\n",
    "1. __[Touchdesigner 64-Bit Build 2021.16410](https://derivative.ca/download)__\n",
    "2. __[Python 3.7.2](https://www.python.org/downloads/release/python-372/)__\n",
    "\n",
    "\n",
    "## Installing Mediapipe and Touchdesign\n",
    "1. Open command prompt and enter following command\n",
    "```bash\n",
    "pip install mediapipe\n",
    "```\n",
    "2. Open Touchdesigner and navigate to File>Preferences and enter path to python site-packages on your system usually. For windows the default is in following directory (change username to yours).\n",
    "```bash\n",
    "C:\\Users\\<username>\\AppData\\Local\\Programs\\Python\\Python37\\Lib\\site-packages\n",
    "```\n",
    "\n",
    "3. Uncheck **Search External Python Path Last**\n",
    "![001.touchdesigner-preferences.png](img/001.touchdesigner-preferences.png)\n",
    "\n",
    "4. Save the preferences and restart Touchdesigner.\n",
    "\n",
    "5. Open Dialogs>Texport and DATs (shortcut alt + t) and enter following command to verify mediapipe installation\n",
    "```python\n",
    "import mediapipe as mp\n",
    "print(dir(mp.solutions))\n",
    "```\n",
    "```bash\n",
    "output:\n",
    "['__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', 'download_utils', 'drawing_styles', 'drawing_utils', 'face_detection', 'face_mesh', 'face_mesh_connections', 'hands', 'hands_connections', 'holistic', 'mediapipe', 'objectron', 'pose', 'pose_connections', 'selfie_segmentation']\n",
    "```\n",
    "\n",
    "Installation of mediapipe is ready and can be used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8250a1",
   "metadata": {},
   "source": [
    "## Sample Program\n",
    "### Face Detection Using Mediapipe in Touchdesigner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "41fd99c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba8564a",
   "metadata": {},
   "source": [
    "#### Get Realtime Webcam Feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "325115c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    cv2.imshow('Raw Webcam Feed', frame)\n",
    "    \n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9fb02052",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c048317",
   "metadata": {},
   "source": [
    "#### Using Face Detection in Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5acd55d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_face_detection = mp.solutions.face_detection\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "with mp_face_detection.FaceDetection(\n",
    "    model_selection=0, min_detection_confidence=0.5) as face_detection:\n",
    "  while cap.isOpened():\n",
    "    success, image = cap.read()\n",
    "    if not success:\n",
    "      print(\"Ignoring empty camera frame.\")\n",
    "      # If loading a video, use 'break' instead of 'continue'.\n",
    "      continue\n",
    "\n",
    "    # To improve performance, optionally mark the image as not writeable to\n",
    "    # pass by reference.\n",
    "    image.flags.writeable = False\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = face_detection.process(image)\n",
    "\n",
    "    # Draw the face detection annotations on the image.\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    if results.detections:\n",
    "      for detection in results.detections:\n",
    "        mp_drawing.draw_detection(image, detection)\n",
    "    \n",
    "    # Flip the image horizontally for a selfie-view display.\n",
    "    cv2.imshow('MediaPipe Face Detection', cv2.flip(image, 1))\n",
    "    \n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ac0308",
   "metadata": {},
   "source": [
    "#### Using Face Detection in Touchdesigner (CHOP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d896ad4",
   "metadata": {},
   "source": [
    "Open face_detection.toe inside toe folder for sample. Using CHOP Script is useful if we want to extract position data from mediapipe such as position of landmark. \n",
    "\n",
    "Code snippet from CHOP Script\n",
    "```python\n",
    "# me - this DAT\n",
    "# scriptOp - the OP which is cooking\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import sys\n",
    "import mediapipe as mp\n",
    "\n",
    "mp_face = mp.solutions.face_detection\n",
    "face_detection = mp_face.FaceDetection(\n",
    "    model_selection=0,\n",
    "    min_detection_confidence=0.7\n",
    ")\n",
    "\n",
    "# press 'Setup Parameters' in the OP to call this function to re-create the parameters.\n",
    "def onSetupParameters(scriptOp):\n",
    "    page = scriptOp.appendCustomPage('Custom')\n",
    "    topPar = page.appendTOP('Face', label='Image with face')\n",
    "    return\n",
    "\n",
    "# called whenever custom pulse parameter is pushed\n",
    "def onPulse(par):\n",
    "\treturn\n",
    "\n",
    "def onCook(scriptOp):\n",
    "    scriptOp.clear()\n",
    "    topRef = scriptOp.par.Face.eval()\n",
    "    \n",
    "    num_faces = 0\n",
    "    max_area = sys.float_info.min\n",
    "    width = 0\n",
    "    height = 0\n",
    "    xmin = 0.5\n",
    "    ymin = 0.5\n",
    "    lx = sys.float_info.max\n",
    "    ly = sys.float_info.max\n",
    "    rx = sys.float_info.max\n",
    "    ry = sys.float_info.max\n",
    "    \n",
    "    if topRef:\n",
    "        img = topRef.numpyArray(delayed=True)\n",
    "        frame = cv2.cvtColor(img, cv2.COLOR_RGBA2RGB)\n",
    "        frame *= 255\n",
    "        frame = frame.astype('uint8')\n",
    "        results = face_detection.process(frame)\n",
    "        \n",
    "        if results.detections:\n",
    "            num_faces = len(results.detections)\n",
    "            \n",
    "            for face in results.detections:\n",
    "            \tarea = face.location_data.relative_bounding_box.width * face.location_data.relative_bounding_box.height\n",
    "            \tif area > max_area:\n",
    "            \t\twidth = face.location_data.relative_bounding_box.width\n",
    "            \t\theight = face.location_data.relative_bounding_box.height\n",
    "            \t\txmin = face.location_data.relative_bounding_box.xmin + width/2.0\n",
    "            \t\tymin = 1 - (face.location_data.relative_bounding_box.ymin + height/2.0)\n",
    "            \t\tlx = face.location_data.relative_keypoints[0].x\n",
    "            \t\tly = 1 - face.location_data.relative_keypoints[0].y\n",
    "            \t\trx = face.location_data.relative_keypoints[1].x\n",
    "            \t\try = 1 - face.location_data.relative_keypoints[1].y\n",
    "            \t\tmax_area = area\n",
    "        \n",
    "    tf = scriptOp.appendChan('face')\n",
    "    tw = scriptOp.appendChan('width')\n",
    "    th = scriptOp.appendChan('height')\n",
    "    tx = scriptOp.appendChan('tx')\n",
    "    ty = scriptOp.appendChan('ty')\n",
    "    leftx = scriptOp.appendChan('left_eye_x')\n",
    "    lefty = scriptOp.appendChan('left_eye_y')\n",
    "    rightx = scriptOp.appendChan('right_eye_x')\n",
    "    righty = scriptOp.appendChan('right_eye_y')\n",
    "    \n",
    "    tf.vals = [num_faces]\n",
    "    tw.vals = [width]\n",
    "    th.vals = [height]\n",
    "    tx.vals = [xmin]\n",
    "    ty.vals = [ymin]\n",
    "    \n",
    "    leftx.vals = [lx]\n",
    "    lefty.vals = [ly]\n",
    "    rightx.vals = [rx]\n",
    "    righty.vals = [ry]\n",
    "    \n",
    "    scriptOp.rate = me.time.rate\n",
    "\n",
    "    return\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b654c5b2",
   "metadata": {},
   "source": [
    "#### Using Face Mesh in Touchdesigner (TOP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48eeb2d5",
   "metadata": {},
   "source": [
    "```python\n",
    "# me - this DAT\n",
    "# scriptOp - the OP which is cooking\n",
    "\n",
    "import numpy\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "\n",
    "point_spec = mp_drawing.DrawingSpec(\n",
    "\tcolor=(0, 100, 255),\n",
    "    thickness=1,\n",
    "    circle_radius=1\n",
    ")\n",
    "line_spec = mp_drawing.DrawingSpec(\n",
    "\tcolor=(255, 200, 0),\n",
    "    thickness=2,\n",
    "    circle_radius=1\n",
    ")\n",
    "face_mesh = mp_face_mesh.FaceMesh(\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "\n",
    "# press 'Setup Parameters' in the OP to call this function to re-create the parameters.\n",
    "def onSetupParameters(scriptOp):\n",
    "\treturn\n",
    "\n",
    "# called whenever custom pulse parameter is pushed\n",
    "def onPulse(par):\n",
    "\treturn\n",
    "\n",
    "\n",
    "def onCook(scriptOp):\n",
    "\tinput = scriptOp.inputs[0].numpyArray(delayed=True)\n",
    "\tif input is not None:\n",
    "\t\tframe = input * 255\n",
    "\t\tframe = frame.astype('uint8')\n",
    "\t\tframe = cv2.cvtColor(frame, cv2.COLOR_RGBA2RGB)\n",
    "\t\t\n",
    "        results = face_mesh.process(frame)\n",
    "\t\t\n",
    "        if results.multi_face_landmarks:\n",
    "\t\t\tfor face_landmarks in results.multi_face_landmarks:\n",
    "\t\t\t\tmp_drawing.draw_landmarks(\n",
    "\t\t\t\t\timage=frame,\n",
    "\t\t\t\t\tlandmark_list=face_landmarks,\n",
    "\t\t\t\t\tconnections=mp_face_mesh.FACEMESH_TESSELATION,\n",
    "\t\t\t\t\tlandmark_drawing_spec=point_spec,\n",
    "\t\t\t\t\tconnection_drawing_spec=line_spec)\n",
    "\n",
    "\t\tframe = cv2.cvtColor(frame, cv2.COLOR_RGB2RGBA)\n",
    "\t\tscriptOp.copyNumpyArray(frame)\n",
    "\treturn\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec9a52c",
   "metadata": {},
   "source": [
    "#### Using Hands in Touchdesigner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fc37a2",
   "metadata": {},
   "source": [
    "```python\n",
    "# me - this DAT\n",
    "# scriptOp - the OP which is cooking\n",
    "\n",
    "import numpy\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# setup for mediapipe\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(\tmodel_complexity=0,\n",
    "                        min_detection_confidence=0.5,\n",
    "                        min_tracking_confidence=0.5)\n",
    "\n",
    "# press 'Setup Parameters' in the OP to call this function to re-create the parameters.\n",
    "def onSetupParameters(scriptOp):\n",
    "    return\n",
    "\n",
    "# called whenever custom pulse parameter is pushed\n",
    "def onPulse(par):\n",
    "    return\n",
    "\n",
    "def onCook(scriptOp):\n",
    "    input = scriptOp.inputs[0].numpyArray(delayed=True)\n",
    "\n",
    "    if input is not None:\n",
    "        image = input * 255\n",
    "        image = image.astype('uint8')\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGBA2RGB)\n",
    "    \n",
    "        results = hands.process(image)\n",
    "        \n",
    "        # Draw the hand annotations on the image.\t\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    image,\n",
    "                    hand_landmarks,\n",
    "                    mp_hands.HAND_CONNECTIONS,\n",
    "                    mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                    mp_drawing_styles.get_default_hand_connections_style())\n",
    "        \n",
    "        scriptOp.copyNumpyArray(image)\n",
    "    \n",
    "    return\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bf75b8",
   "metadata": {},
   "source": [
    "#### Using Hands in Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "152ae528",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "# For webcam input:\n",
    "cap = cv2.VideoCapture(0)\n",
    "with mp_hands.Hands(\n",
    "    model_complexity=0,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5) as hands:\n",
    "  while cap.isOpened():\n",
    "    success, image = cap.read()\n",
    "    if not success:\n",
    "      print(\"Ignoring empty camera frame.\")\n",
    "      # If loading a video, use 'break' instead of 'continue'.\n",
    "      continue\n",
    "\n",
    "    # To improve performance, optionally mark the image as not writeable to\n",
    "    # pass by reference.\n",
    "    image.flags.writeable = False\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(image)\n",
    "\n",
    "    # Draw the hand annotations on the image.\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    if results.multi_hand_landmarks:\n",
    "      for hand_landmarks in results.multi_hand_landmarks:\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image,\n",
    "            hand_landmarks,\n",
    "            mp_hands.HAND_CONNECTIONS,\n",
    "            mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "            mp_drawing_styles.get_default_hand_connections_style())\n",
    "    \n",
    "    # Flip the image horizontally for a selfie-view display.\n",
    "    cv2.imshow('MediaPipe Hands', cv2.flip(image, 1))\n",
    "    \n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "      break\n",
    "    \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95dfdc29",
   "metadata": {},
   "source": [
    "#### Notes on Using Mediapipe in Touchdesigner\n",
    "\n",
    "1. It is relatively easy to install and setup mediapipe for touchdesigner\n",
    "2. Integration only require you to specify path to mediapipe installation (site-packages folder)\n",
    "3. Touchdesigner has built-in python version 3.7.2, to minimize compatibility issue I think it is recommended to install python 3.7.2 in your system and install mediapipe globally in your system.\n",
    "4. I noticed performance drop when executing mediapipe code from touchdesigner. I used Intel Core i7 8550U with NVDIA MX150 and 8 GB of RAM. I saw 7-10 fps drop when executing same code in touchdesigner compared to executing it from command prompt. It is likely to another process running in touchdesigner, I expect it doesn't happen in better PC/laptop.\n",
    "5. Example from mediapipe website is good enough to start using the models but some details is missing from the doc. It is not a big deal because we can easily find the answers online or by reading the code from mediapipe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58638da7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
